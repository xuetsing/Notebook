# 矩阵求导
## 向量对向量求导:
<div align="center"> <img src="/pic/math-1.png"/> </div>
<div align="center"> <img src="/pic/math-2.png"/> </div>

## 标量对向量求导:
<div align="center"> <img src="/pic/math-3.png"/> </div>
<div align="center"> <img src="/pic/math-4.png"/> </div>

## 标量对矩阵求导
<div align="center"> <img src="/pic/math-5.png"/> </div>


# 线性回归
<div align="center"> <img src="/pic/r-1.png"/> </div>
<div align="center"> <img src="/pic/r-2.png"/> </div>

## 似然函数：
<div align="center"> <img src="/pic/r-3.png"/> </div>

## 高斯的对数似然与最小二乘：
<div align="center"> <img src="/pic/r-4.png"/> </div>

最小二乘估计：又称最小平方法，是一种数学优化技术。它通过最小化误差的平方和寻找数据的最佳函数匹配。

## θ的解析式的求解过程：
<div align="center"> <img src="/pic/r-5.png"/> </div>

## 最小二乘意义下的参数最优解：
<div align="center"> <img src="/pic/r-6.png"/> </div>

## 加入λ扰动后：
<div align="center"> <img src="/pic/r-7.png"/> </div>
<div align="left"> <img src="/pic/r-8.png"/> </div>

## 梯度下降法
<div align="center"> <img src="/pic/r-9.png"/> </div>
<div align="center"> <img src="/pic/r-10.png"/> </div>
<div align="center"> <img src="/pic/r-11.png"/> </div>

# logistic 回归
## 1. sigmoid函数及其导数:
<div align="center"> <img src="/pic/lr-0.png"/> </div>

## 2. 极大似然函数L(θ)：
<div align="center"> <img src="/pic/lr-2.png"/> </div>

## 3. 对L(θ)取对数：
<div align="center"> <img src="/pic/lr-33.png"/> </div>

## 4. 对l(θ)求导：
<div align="center"> <img src="/pic/lr-4.png"/> </div>

## 5. 参数更新：
<div align="center"> <img src="/pic/lr-5.png"/> </div>

## 6. y∈{0, 1}时的损失函数：
<div align="center"> <img src="/pic/lr-6.png"/> </div>

## 7. y∈{-1， 1}时的损失函数：
<div align="center"> <img src="/pic/lr-7.png"/> </div>

## 8. Softmax回归：
<div align="center"> <img src="/pic/lr-8.png"/> </div>


# SVM
## 输入数据：
<div align="center"> <img src="/pic/svm-1.png"/> </div>
<div align="center"> <img src="/pic/svm-2.png"/> </div>

## 推导目标函数：
<div align="center"> <img src="/pic/svm-3.png"/> </div>
<div align="center"> <img src="/pic/svm-4.png"/> </div>

## 建立目标函数：
<div align="center"> <img src="/pic/svm-5.png"/> </div>
<div align="center"> <img src="/pic/svm-6.png"/> </div>

## 拉格朗日乘子法：
<div align="center"> <img src="/pic/svm-7.png"/> </div>
<div align="center"> <img src="/pic/svm-80.png"/> </div>

## 计算拉格朗日函数的对偶函数：
<div align="center"> <img src="/pic/svm-9.png"/> </div>
<div align="center"> <img src="/pic/svm-10.png"/> </div>
<div align="left"> <img src="/pic/svm-110.png"/> </div>
<div align="center"> <img src="/pic/svm-12.png"/> </div>

## 整理目标函数，添加负号，构造并求解约束最优化问题：
<div align="center"> <img src="/pic/svm-15.png"/> </div>
<div align="center"> <img src="/pic/svm-16.png"/> </div>


# XGBoost
## 对损失函数Loss进行二阶泰勒展开，并加入了正则项，防止过拟合：
<div align="center"> <img src="/pic/xgb-1.png"/> </div>

## 在xgboost中，树的定义为：树由结构部分q和叶子分数部分w组成：其中q(x)代表第几个叶子结点，而Wq(x)表示在此叶子结点下分数是多少，即叶权值。
<div align="center"> <img src="/pic/xgb-5.png"/> </div>

## 树的复杂度(不是唯一定义)：叶结点总数和叶权值平方和的加权。
<div align="center"> <img src="/pic/xgb-20.png"/> </div>
<div align="center"> <img src="/pic/xgb-21.png"/> </div>

## 目标函数计算：
<div align="center"> <img src="/pic/xgb-3.png"/> </div>

## 目标函数继续化简：
<div align="center"> <img src="/pic/xgb-4.png"/> </div>

## 寻求Gain最大的划分点：
<div align="center"> <img src="/pic/xgb-6.png"/> </div>

## 总结：
* 如何进行子树划分：
	* 借鉴ID3/C4.5/CART的做法，使用贪心法：
	* 对于某可行划分，计算划分后的J(f)；
	* 对于所有可行划分，选择J(f)降低最小的分割点。
* 建树方法：
	* 第一种：枚举可行的分割点，选择增益最大的划分，继续同样的操作，直到满足某阈值或得到纯结点。
	* 第二种：近似，抽取一些分裂点来作比较。
* xgb怎么给特征评分：
	* 在训练过程中，通过Gini指数选择分离点的特征，一个特征被选中的次数越多，那么该特征评分越高。
* xgb用泰勒展开的优势：
	* 使用一阶和二阶偏导，二阶导数有利于梯度下降的更快更准，收敛快，精度高。



# Adaboost
<div align="center"> <img src="/pic/ab-1.png"/> </div>
<div align="center"> <img src="/pic/ab-2.png"/> </div>
<div align="center"> <img src="/pic/ab-3.png"/> </div>
<div align="center"> <img src="/pic/ab-4.png"/> </div>

# GBDT 梯度提升决策树
* GBDT总结：
	* 损失函数是最小平方误差、绝对值误差等，则为回归问题。而误差函数换成多类别logistic似然函数，则成为分类问题。
	* Adaboost是利用错误率来更新样本权重和基学习器权重。而GBDT是利用残差来拟合CART树来逼近真实值。




