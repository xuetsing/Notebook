## 反向传播算法
<div align="center"> <img src="/pic/bp-2.png"/> </div>
<div align="center"> <img src="/pic/bp-3.png"/> </div>
<div align="center"> <img src="/pic/bp-4.png"/> </div>
<div align="center"> <img src="/pic/bp-1.png"/> </div>

## 批规范化操作（Batch Normalization）
* 解决梯度弥散的问题，加快模型收敛速度。
* 解决内部协变量偏移的问题， 协变量偏移是指对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大越来越大，不过它们的样本标签任然保持不变，即源空间和目标空间的数据分布不一致，其条件概率一致但是边缘概率不同。
<div align="center"> <img src="/pic/bn-2.png"/> </div>
<div align="center"> <img src="/pic/bn-1.png"/> </div>

# 优化算法
## SGD（随机梯度下降法）
<div align="center"> <img src="/pic/optimization-1.png"/> </div>

## Momentum（基于动量的随机梯度下降法）
<div align="center"> <img src="/pic/optimization-2.png"/> </div>

## Nesterov型动量随机下降法
<div align="center"> <img src="/pic/optimization-3.png"/> </div>

## Adagrad法
<div align="center"> <img src="/pic/optimization-4.png"/> </div>

## Adadelta法
<div align="center"> <img src="/pic/optimization-5.png"/> </div>

## RMSProp法
<div align="center"> <img src="/pic/optimization-6.png"/> </div>

## Adam法
<div align="center"> <img src="/pic/optimization-7.png"/> </div>
<div align="center"> <img src="/pic/optimization-8.png"/> </div>


# CNN模型压缩
* 背景：
	1. 深度网络巨大的存储代价和计算开销。
	2. 深度网络的过参数化，模型内部参数存在着巨大的冗余。
* 技术分类：
	1. 前端压缩：不改变原网络结构。主要包括知识蒸馏、紧凑的模型结构设计以及滤波器层面的剪枝等。
	2. 后端压缩：尽可能地减少模型大小，会对原始网络结构造成极大程度的改造。包括低秩近似、未加限制的剪枝、参数量化以及二值网络等。
## 低秩近似
<div align="center"> <img src="/pic/compress-1.png"/> </div>
<div align="center"> <img src="/pic/compress-2.png"/> </div>
<div align="center"> <img src="/pic/compress-3.png"/> </div>

## 剪枝与稀疏约束
<div align="center"> <img src="/pic/compress-4.png"/> </div>
<div align="center"> <img src="/pic/compress-5.png"/> </div>
<div align="center"> <img src="/pic/compress-6.png"/> </div>
<div align="center"> <img src="/pic/compress-7.png"/> </div>
<div align="center"> <img src="/pic/compress-8.png"/> </div>
<div align="center"> <img src="/pic/compress-9.png"/> </div>
<div align="center"> <img src="/pic/compress-10.png"/> </div>
<div align="center"> <img src="/pic/compress-11.png"/> </div>
<div align="center"> <img src="/pic/compress-12.png"/> </div>
<div align="center"> <img src="/pic/compress-13.png"/> </div>

## 参数量化
<div align="center"> <img src="/pic/compress-14.png"/> </div>
<div align="center"> <img src="/pic/compress-15.png"/> </div>
<div align="center"> <img src="/pic/compress-16.png"/> </div>
<div align="center"> <img src="/pic/compress-18.png"/> </div>
<div align="center"> <img src="/pic/compress-17.png"/> </div>
<div align="center"> <img src="/pic/compress-19.png"/> </div>
<div align="center"> <img src="/pic/compress-20.png"/> </div>

## 二值网络
<div align="center"> <img src="/pic/compress-21.png"/> </div>
<div align="center"> <img src="/pic/compress-22.png"/> </div>
<div align="center"> <img src="/pic/compress-23.png"/> </div>
<div align="center"> <img src="/pic/compress-24.png"/> </div>
<div align="center"> <img src="/pic/compress-25.png"/> </div>
<div align="center"> <img src="/pic/compress-26.png"/> </div>

## 知识蒸馏
<div align="center"> <img src="/pic/compress-27.png"/> </div>
<div align="center"> <img src="/pic/compress-28.png"/> </div>
<div align="center"> <img src="/pic/compress-29.png"/> </div>
<div align="center"> <img src="/pic/compress-30.png"/> </div>

## 紧凑的网络结构
<div align="center"> <img src="/pic/compress-31.png"/> </div>
<div align="center"> <img src="/pic/compress-32.png"/> </div>
<div align="center"> <img src="/pic/compress-33.png"/> </div>
<div align="center"> <img src="/pic/compress-34.png"/> </div>


# 参数初始化
## 全零初始化
* 所有参数都初始化为0。
* 问题：参数全为0时网络不同神经元的输出必然相同，相同输出则导致梯度更新完全一样，这样便会令更新后的参数任然保持一样的状态。

## 随机初始化
<div align="center"> <img src="/pic/initialization-1.png"/> </div>
* 问题：网络输出数据分布的方差会随着输入神经元个数改变。

## Xavier参数初始化方法
<div align="center"> <img src="/pic/initialization-2.png"/> </div>
* 具体分析有下式：（其中s为未经非线性变换的该层网络输出结果， w为该层参数，x为该层输入数据）。
<div align="center"> <img src="/pic/initialization-3.png"/> </div>
* 改进：在初始化的同时加上对方差大小的规范化，维持了输入输出数据分布方差的一致性。
* 问题：未考虑非线性映射函数对输入的影响。因为使用如ReLU函数等非线性映射函数后，输出数据的期望往往不再为0。

## He参数初始化方法
* 改进：将非线性映射造成的影响考虑进参数初始化中。他们提出原本Xavier方法中方差规范化的分母应为sqrt(n/2)而不是sqrt(n)。

## 参数初始化服从均匀分布
<div align="center"> <img src="/pic/initialization-4.png"/> </div>
<div align="center"> <img src="/pic/initialization-5.png"/> </div>


# 目标函数
## 分类任务的目标函数
<div align="center"> <img src="/pic/loss_fun-1.png"/> </div>

## 交叉熵损失函数
<div align="center"> <img src="/pic/loss_fun-2.png"/> </div>

## 合页损失函数
<div align="center"> <img src="/pic/loss_fun-3.png"/> </div>

## 坡道损失函数
<div align="center"> <img src="/pic/loss_fun-4.png"/> </div>
<div align="center"> <img src="/pic/loss_fun-50.png"/> </div>
<div align="center"> <img src="/pic/loss_fun-6.png"/> </div>

## 大间隔交叉熵损失函数

## 中心损失函数

## 回归任务的目标函数
<div align="center"> <img src="/pic/loss_fun-7.png"/> </div>
<div align="center"> <img src="/pic/loss_fun-8.png"/> </div>

## l1损失函数
<div align="center"> <img src="/pic/loss_fun-9.png"/> </div>

## l2损失函数
<div align="center"> <img src="/pic/loss_fun-10.png"/> </div>

## Tukey's biweight损失函数
<div align="center"> <img src="/pic/loss_fun-11.png"/> </div>
<div align="center"> <img src="/pic/loss_fun-12.png"/> </div>
<div align="center"> <img src="/pic/loss_fun-13.png"/> </div>

## 其他任务的目标函数：标记分布
<div align="center"> <img src="/pic/loss_fun-14.png"/> </div>
<div align="center"> <img src="/pic/loss_fun-16.png"/> </div>
<div align="center"> <img src="/pic/loss_fun-17.png"/> </div>
<div align="center"> <img src="/pic/loss_fun-15.png"/> </div>

# 网络正则化
* 目的：防止网络过拟合，提升其泛化能力。
## l2正则化
<div align="center"> <img src="/pic/regularization-1.png"/> </div>

## l1正则化
<div align="center"> <img src="/pic/regularization-2.png"/> </div>

## 最大范数约束
<div align="center"> <img src="/pic/regularization-3.png"/> </div>

## 随机失活
* 对于某层的每个神经元，在训练阶段均以概率p随机将该神经元权重置0，测试阶段所有神经元均呈激活态，但其权重需乘（1-p）以保证训练和测试阶段各自权重拥有相同的期望。
<div align="center"> <img src="/pic/regularization-4.png"/> </div>

## 验证集的使用
* 在模型训练前从训练集数据随机划分出一个子集作为验证集，用以在训练阶段评测模型预测性能，检验模型的泛化能力。
* 借助验证集对网络训练“早停（early stopping）”也是一种有效的防止网络过拟合方法，可取验证集准确率最高的那一轮训练结果作为最终网络，用于测试集数据的预测。




