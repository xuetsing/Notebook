# 分类模型
## Inception V1
* Split-Merge结构:1x1卷积，3x3卷积，5x5卷积，3x3池化
	* 增加网络宽度
	* 增加网络对多尺度的适应性
	* 串联合并所有分支输出
* Bottleneck Layer：
	* 使用NiN的1x1卷积进行特征降维
* 取消全连接层
	* 由全局平均池化替代（Global average pooling）
* 2个辅助分类器：
	* 解决前几层的梯度消失问题
	* 有效加速收敛
	* 测试阶段不使用

## Inception V2
* 引入批归一化（Batch Normalization）
	* 解决内部协变量偏移(Internal Covariate Shift)问题
	* 白化：使每一层的输出都规范到N(0,1)
	* 允许较高的学习率
	* 取代部分Dropout
	* 在batch范围内，对每个特征通道分别进行归一化（减均值&除以标准差）
	* 训练阶段：实时计算
	* 测试阶段：使用固定值（对训练求平均）
	* 位置：卷积→BN→ReLU
* 5x5卷积核→2个3x3卷积核

## Inception V3
* 非对称卷积：NxN 分解成1xN→Nx1
	* 降低参数数量和计算量
* 高效的降尺寸：
	* 降尺寸后增加特征通道
* 2个并行分支：
	* 卷积分支+池化分支
	* 串联分支结果
* 取消浅层的辅助分类器：完全无用
* 深层辅助分类器只在训练后期有用
	* 加上BN和Dropout，主分类器Top1性能提升0.4%
	* 正则化作用
	* 用在最后一层17x17后

## ResNet
* Skip/shortcut connection
* 全是3x3卷积核，卷积步长2取代池化，使用Batch Normalization
* 取消：max池化，全连接层，Dropout
* 使用Bottleneck优化残差映射网络

## Inception V4
* 引入残差到Inception结构中

## ResNeXt
* 提出提出第三个DNN维度cardinality基数
	* 增加cardinality基数会不断提高性能
* 采用Split-Transform-Aggregate策略
	* 将卷积核按通道分组，形成32个并行分支
	* 低维度卷积进行特征变换
	* 加法合并
* 同参数规模下，增加结构，提高模型表达力
	* 100层ResNeXt = 200层ResNet

## CNN设计准则
* 避免信息瓶颈
	* 空间尺寸HxW会变小
	* 输出通道数C会变多
	* 数据量HxWxC要缓慢变小
* 感受野要足够大，使用多个小尺寸卷积核代替一个大尺寸卷积核
	* 可以使用多个非线性激活
	* 参数少，计算快
* 分组策略，降低计算量
	* G组M/G个滤波器代替M个滤波器
* 低秩分解，降低参数和计算量
	* 空间分解
	* 通道分解


# 目标检测
#### 分类:
* 候选区域/窗 + 深度学习分类：
	1. R-CNN（Selective Search + CNN + SVM）
	2. SPP-net（ROI Pooling）
	3. Fast R-CNN（Selective Search + CNN + ROI）
	4. Faster R-CNN（RPN + CNN + ROI）
	5. R-FCN
* 基于深度学习的回归方法
	1. YOLO
	2. SSD

## R-CNN
* 简要步骤：
	1. 输入测试图像。
	2. 利用选择性搜索(Selective Search)算法在图像中从下到上提取2000个左右的可能包含物体的候选区域(Region Rroposal)。
	3. 因为取出的区域大小各自不同，所以需要将每个Region Proposal缩放(warp)成统一的227x227的大小并输入到CNN，将CNN的fc7层的输出作为特征。
	4. 将每个Region Proposal提取到的CNN特征输入到SVM进行分类。
* 具体步骤：
	1. 训练一个分类模型(比如AlexNet)。
	2. 对该模型做fine-tuning：
		1. 将分类数从1000改为21，比如20个物体类别 + 1个背景
		2. 去掉最后一个全连接层
	3. 特征提取：
		1. 提取图像的所有候选框(Selective Search)
		2. 对于每一个区域：修正区域大小以适合CNN的输入，做一次前向运算，将第五个池化层的输出(对候选框提取到的特征)存到硬盘。
	4. 训练一个SVM分类器(二分类)来判断这个候选框里物体的类别，每个类别对应一个SVM，判断是不是属于这个类别，是就算positive，反之nagative。
	5. 使用回归器精细修正候选框位置，对于每一个类，训练一个线性回归模型去判定这个框是否框得完美。
* 问题：
	1. Selective Search提取的候选框多达2000个左右，每个框都要进行CNN提特征+SVM分类，计算量很大。

## SPP Net(Spatial Pyramid Pooling)
* 特点：
	1. 结合空间金字塔方法实现CNN的多尺度输入。SPP Net的第一个贡献就是在最后一个卷积层后，接入了金字塔池化层，保证传到下一层全连接层的输入固定。
	2. 只对原图提取一次卷积特征。只对原图进行一次卷积计算，便得到整张图的卷积特征feature map，然后找到每个候选框在feature map上的映射patch，将此patch作为每个候选框的卷积特征输入到SPP layer和之后的层，完成特征提取工作。
* 与R-CNN比较：
	1. R-CNN要对每个区域计算卷积，而SPPNet只需要计算一次卷积，从而节省了大量的计算时间，比R-CNN有一百倍左右的提速。

## Fast R-CNN
* 与R-CNN比较：
	1. 最后一个卷积层后加了一个ROI pooling layer，把不同大小的输入映射到一个固定尺度的特征向量。
		1. ROI pooling layer实际上是SPP-NET的一个精简版，SPP-NET对每个proposal使用了不同大小的金字塔映射，而ROI pooling layer只需要下采样到一个7x7的特征图。对于VGG16网络conv5_3有512个特征图，这样所有region proposal对应了一个7*7*512维度的特征向量作为全连接层的输入。
	2. 损失函数使用了多任务损失函数(multi-task loss)，将边框回归Bounding Box Regression直接加入到CNN网络中训练。
	3. 处理流程：
		1. R-CNN：先提许多的候选框，然后CNN提取每个候选框的特征，之后用SVM分类器，最后再做bbox regression。
		2. Fast R-CNN：先对一张完整图片CNN提取特征，得到每张候选框的映射特征，之后softmax分类和回归同步训练。
	4. Fast R-CNN直接使用softmax替代SVM分类，同时利用多任务损失函数边框回归也加入到了网络中，这样整个的训练过程是端到端的(除去Region Proposal提取阶段)。
* 问题：
	1. 选择性搜索(Selective Search)，找到所有的候选框，非常耗时。

## Faster R-CNN
* 步骤：
	1. 对整张图片用CNN提特征，得到feature map。
	2. CNN特征输入到RPN，得到候选框的特定信息。
	3. 对候选框中提取出的特征，使用分类器判别是否属于一个特定类。
	4. 对于属于某一类别的候选框，用回归器进一步调整其位置。
* RPN：
	1. 在feature map上滑动窗口
	2. 建一个神经网络用于物体分类+框位置的回归
	3. 滑动窗口的位置提供了物体的大体位置信息
	4. 框的回归提供了框更精确的位置
* anchor：
	* Anchor给出一个基准窗大小，按照倍数和长宽比例得到不同大小的窗。论文中基准窗大小为16，给了（8、16、32）三种倍数和（1:2、1:1、2:1）三种比例，能够得到一共9种尺度的anchor
* 联合训练：
	1. 单独训练RPN网络，网络参数由预训练模型载入；
	2. 单独训练Fast-RCNN网络，将第一步RPN的输出候选区域作为检测网络的输入。具体而言，RPN输出一个候选框，通过候选框截取原图像，并将截取后的图像通过几次conv-pool，然后再通过roi-pooling和fc再输出两条支路，一条是目标分类softmax，另一条是bbox回归。截止到现在，两个网络并没有共享参数，只是分开训练了；
	3. 再次训练RPN，此时固定网络公共部分的参数，只更新RPN独有部分的参数；
	4. 用RPN的结果再次微调Fast-RCNN网络，固定网络公共部分的参数，只更新Fast-RCNN独有部分的参数。
* Faster R-CNN的主要贡献：
	1. 引入提取候选区域的网络RPN(Region Proposal Network)替代费时的选择性搜索(Selective Search)。
	2. 通过交替训练，使RPN和Fast R-CNN网络共享参数
	3. 引入anchor box应对目标形状的变化问题。（anchor就是位置和大小固定的box，可以理解成事先设置好的固定的proposal）

## YOLO（You Only Look Once）
* 使用了回归的思想，利用整张图作为网络的输入，直接在图像的多个位置上回归出这个位置的目标边框，以及目标所属的类别。
* 步骤：
	1. 给一个输入图像，首先将图像划分成7*7的网格
	2. 对于每个网格，我们都预测2个边框（包括每个边框是目标的置信度以及每个边框区域在多个类别上的概率）
	3. 根据上一步可以预测出7*7*2个目标窗口，然后根据阈值去除可能性比较低的目标窗口，最后NMS去除冗余窗口即可
* 小结：
	1. YOLO将目标检测任务转换成一个回归问题，大大加快了检测的速度，使得YOLO可以每秒处理45张图像。而且由于每个网络预测目标窗口时使用的是全图信息，使得false positive比例大幅降低（充分的上下文信息）。
* 问题：
	1. 没有了Region Proposal机制，只使用7*7的网格回归会使得目标不能非常精准的定位，这也导致了YOLO的检测精度并不是很高。

## SSD（Single Shot MultiBox Detector）
* 小结：
	1. SSD获取目标位置和类别的方法跟YOLO一样，都是使用回归，但是YOLO预测某个位置使用的是全图的特征，SSD预测某个位置使用的是这个位置周围的特征（感觉更合理一些）。
	2. SSD使用Faster R-CNN的anchor机制建立某个位置和其特征的对应关系。不同于Faster R-CNN，这个anchor是在多个feature map上，这样可以利用多层的特征并且自然的达到多尺度（不同层的feature map 3*3滑窗感受野不同）。
	3. SSD结合了YOLO中的回归思想和Faster R-CNN中的anchor机制，使用全图各个位置的多尺度区域特征进行回归，既保持了YOLO速度快的特性，也保证了窗口预测的跟Faster R-CNN一样比较精准。SSD在VOC2007上mAP可以达到72.1%，速度在GPU上达到58帧每秒。